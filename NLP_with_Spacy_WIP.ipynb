{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/archaeogeek/colab/blob/main/NLP_with_Spacy_WIP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tnrBNLGeu-f"
      },
      "source": [
        "Setting up working environment.\n",
        "Book says spaCy 2.0 or later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLVEgy6hekiy"
      },
      "source": [
        "!pip install spacy\n",
        "!pip install pytextrank --upgrade\n",
        "!python -m spacy info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpYeplT6fT3x"
      },
      "source": [
        "Install statistical models for spaCy. Models have naming syntax lang_type_genre_size. Downloading 'en' downloads the default en_core_web_md, which is the smallest general purpose model for the English language"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqkszNmyfTJ9"
      },
      "source": [
        "!python -m spacy download en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SkGV10IgK3j"
      },
      "source": [
        "# Basic NLP operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hj9kFoxfgTiy"
      },
      "source": [
        "**Tokenisation**\n",
        "Parsing text into tokens, which can be words, numbers or punctuation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p73NpzUEjIqy"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u'I am driving to Kendal')\n",
        "print([w.text for w in doc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdlTcPUOlggM"
      },
      "source": [
        "**Lemmatization**\n",
        "The base form of any given token. eg, the Lemma for 'driving' is 'drive'. Should also work for tenses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Awh2MjxVlo00"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u'this dog enjoyed both running and sleeping, in fact it slept very well and runs most days')\n",
        "for token in doc:\n",
        "  print(token.text, token.lemma_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eQjU5CQp-2u"
      },
      "source": [
        "We need to deal with nicknames somehow- we can define them for spaCy to use. It doesn't seem to work if you just call 'doc' in the final line though- you seem to need to call it in the print statement. Maybe you need to define it after you add the tokenizer special case?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZO8eUyf1qGrd"
      },
      "source": [
        "import spacy\n",
        "from spacy.symbols import ORTH, LEMMA\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u'I am driving to Lancaster, in Lancs')\n",
        "print([w.text for w in doc])\n",
        "special_case = [{ORTH: u'Lancs', LEMMA: u'Lancashire'}]\n",
        "nlp.tokenizer.add_special_case(u'Lancs', special_case)\n",
        "print([w.lemma_ for w in nlp(u'I am driving to Lancaster in Lancs')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVyvIS_StHB1"
      },
      "source": [
        "**Part-of-speech Tagging**\n",
        "Tags each word in a sentence with whether it's a noun, verb etc. Standard grammatical constructs, along with punctuation are called coarse-grained parts of speech. Tenses and types of pronoun etc are fine-grained parts of speech. NLP needs to do this for multi-sentence phrases as lemmatization will reduce everything to it's basic form- tense and anything signifying intent will disappear. The following snippet identifies the token that is in the *present progressive* form (`VBG`) or *base* form (`VB`) to identify the part of the phrase signifying intent rather than something that happened in the past."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixjB6lisLYMs"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u'I have driven to Lancaster. Now I am driving to Kendal')\n",
        "print([w.text for w in doc if w.tag_ == 'VBG' or w.tag_ == 'VB'])\n",
        "print([w.text for w in doc if w.pos_ == 'PROPN'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNgCml2bOy0u"
      },
      "source": [
        "**Syntactic Relations** The next step is to link the proper noun with verb that best signifies intent. SpaCy uses syntactic dependency labels to show the relationship between pairs of words in a sentence. Verb phrases are parents, noun phrases are children (but can have children of their own). The relationship between parents or `heads`, and children is one-to-many, so the dependency label (`dobj`) is always assigned to the child\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hL8qgDHYRGGg"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u'I have driven to Lancaster. Now I am walking to Liverpool')\n",
        "for token in doc:\n",
        "  print(token.head.text, token.text, token.pos_, token.dep_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiobE5UySiFm"
      },
      "source": [
        "In the above, looking at the `token.head.text` and the `token.dep_` you can extract the `ROOT` and the `pobj` to extract the intent part of the phrase. We can also split the phrase up into sentences with `doc.sents`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtrMpWfCS9Ic"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u'I have driven to Lancaster. Now I am cycling to Liverpool')\n",
        "for sent in doc.sents:\n",
        "  print([w.text for w in sent if w.dep_ == 'ROOT' or w.dep_ == 'pobj'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgVhFQAsTsBr"
      },
      "source": [
        "**Named Entity Recognition** named entities are real objects such as people, organisations, locations. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1KfD8AIT4GR"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u'I have driven to Lancaster. Now I am walking to Galgate. I have been to Burton-in-Kendal too.')\n",
        "for token in doc:\n",
        "  if token.ent_type !=0:\n",
        "    print(token.text, token.ent_type_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcduUykx4b01"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u'I have driven to Lancaster. Now I am walking to Galgate. I have been to Horndean and Burton-in-Kendal too.')\n",
        "for token in doc:\n",
        "  if token.ent_type !=0:\n",
        "    print(token.text, token.ent_type_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EDav6ZzN9m6"
      },
      "source": [
        "# Gotchas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MUCLesVOK5j"
      },
      "source": [
        "**UK Placenames** If we try a small village with a multi-part name such as *Burton-in-Kendal*, spaCy doesn't recognise it as a single proper noun. So we're going to need to try different entity taggers or dictionaries to find one that can understand more UK places (could be hills, mountains, etc too). If it's a single small place such as *Galgate* that's not in the list, it recognises it as a proper noun (`PROPN`) but doesn't tag it as the object (`pobj`). Then... if we use named entity recognition, it mis-tags *Lancaster* and *Burton* as `PERSON`, which is funny but problematic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a50zMviYWBqx"
      },
      "source": [
        "This might be the point at which we need to use something like [mordecai](https://github.com/openeventdata/mordecai) to do our geoparsing, which will require an ES install..."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternative approach- based on Chapter 10 (Training Models), there is an example that corrects the tag for a location from LOC to GPE. The section \"Automating the Example Creation Process\" takes a text file and a list of entities, and says \"for each entity in this list that appears in the text file, classify as GPE\". We should be able to both the text file and the list from the OS Open Names data and then use that to train the model. Let's see if we can use this approach to add Lancaster as a GPE in the model though..."
      ],
      "metadata": {
        "id": "mPMhN2p8zyhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.lang.char_classes import ALPHA, ALPHA_LOWER, ALPHA_UPPER\n",
        "from spacy.lang.char_classes import CONCAT_QUOTES, LIST_ELLIPSES, LIST_ICONS\n",
        "from spacy.util import compile_infix_regex\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Modify tokenizer infix patterns to stop the tokeniser breaking on hyphens\n",
        "# from https://spacy.io/usage/linguistic-features#native-tokenizers\n",
        "infixes = (\n",
        "    LIST_ELLIPSES\n",
        "    + LIST_ICONS\n",
        "    + [\n",
        "        r\"(?<=[0-9])[+\\\\-\\\\*^](?=[0-9-])\",\n",
        "        r\"(?<=[{al}{q}])\\\\.(?=[{au}{q}])\".format(\n",
        "            al=ALPHA_LOWER, au=ALPHA_UPPER, q=CONCAT_QUOTES\n",
        "        ),\n",
        "        r\"(?<=[{a}]),(?=[{a}])\".format(a=ALPHA),\n",
        "        # âœ… Commented out regex that splits on hyphens between letters:\n",
        "        # r\"(?<=[{a}])(?:{h})(?=[{a}])\".format(a=ALPHA, h=HYPHENS),\n",
        "        r\"(?<=[{a}0-9])[:<>=/](?=[{a}])\".format(a=ALPHA),\n",
        "    ]\n",
        ")\n",
        "\n",
        "infix_re = compile_infix_regex(infixes)\n",
        "nlp.tokenizer.infix_finditer = infix_re.finditer\n",
        "\n",
        "doc = nlp(u'Could you send a taxi to Horndean? I need to get to Lancaster, via Burton-in-Kendal, to meet Richard Burton')\n",
        "train_exams = []\n",
        "places = ['Horndean', 'Burton-in-Kendal', 'Lancaster']\n",
        "for sent in doc.sents:\n",
        "  entities = []\n",
        "  for token in sent:\n",
        "    # sanity check the existing token type and type number before we process\n",
        "    print(token.text, token.ent_type_, token.ent_type)\n",
        "    if token.text in places:\n",
        "      start = token.idx - sent.start_char\n",
        "      # force entries from the list to have an entity type of GPE\n",
        "      # note this doesn't affect the type number which could still be zero\n",
        "      entity = (start, start + len(token), 'GPE')\n",
        "    else:\n",
        "      start = token.idx - sent.start_char\n",
        "      # other entries not in the places list get their existing type and type number\n",
        "      entity = (start, start + len(token), token.ent_type_)\n",
        "    # prevent adding all the entities that still have no entity type\n",
        "    if entity[2] !='':\n",
        "      entities.append(entity)\n",
        "  tpl = (sent.text, {'entities': entities})\n",
        "  train_exams.append(tpl)\n",
        "# print resulting correctly tagged training examples\n",
        "print(train_exams)"
      ],
      "metadata": {
        "id": "CRdJv6a33JJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above approach seems to correctly tag the places and doesn't add all the other scrote to the list of training examples"
      ],
      "metadata": {
        "id": "cGM4vN8Q6IK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spacy Tokenisation\n"
      ],
      "metadata": {
        "id": "QNxmw3FjuxHc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aka how we deal with things like \"London Road\" without splitting it into \"London\" and \"Road\""
      ],
      "metadata": {
        "id": "U2JYsx0pu4SB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The example below shows using `token.rights` and `token.lefts` to pick up the left and right children of a token"
      ],
      "metadata": {
        "id": "tnjaE6MExtfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"I live on Greaves Road in Lancaster but I used to live on Greenfield Street\") #\"bright red apples on the tree\"\n",
        "for sent in doc.sents:\n",
        "  for token in sent:\n",
        "    print(token.text, token.ent_type_, [t.text for t in token.lefts], token.n_lefts, [t.text for t in token.rights], token.n_rights)\n",
        "# print([token.text for token in doc[2].lefts])  # ['bright', 'red']\n",
        "# print([token.text for token in doc[3].rights])  # ['on']\n",
        "print(doc[2].n_lefts)  # 2\n",
        "print(doc[2].n_rights)  # 1"
      ],
      "metadata": {
        "id": "NXnqFO7vvA02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://stackoverflow.com/questions/57206701/spacy-tokenizer-rule-for-exceptions-that-contain-whitespace suggests a couple of approaches. The simplest might be to merge noun chunks?"
      ],
      "metadata": {
        "id": "-WYGwYCWIuhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp.add_pipe(\"merge_noun_chunks\")\n",
        "doc = nlp(u'I live on Greaves Road')\n",
        "for token in doc:\n",
        "  print(token.text, token.pos_, token.dep_)\n"
      ],
      "metadata": {
        "id": "M_BC1mBQJ0PY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding a terminology list to our model"
      ],
      "metadata": {
        "id": "QhKnsea2y_pK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thanks to https://notebook.community/psychemedia/parlihacks/notebooks/Text%20Scraping%20-%20Notes this might finally be the approach for pre-loading our additional location entries! The next step is (I think) to turn this into a custom pipeline that we can load alongside the proper one rather than doing the list mungeing each time"
      ],
      "metadata": {
        "id": "IJD2KxcAzGHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "from spacy import displacy\n",
        "from spacy.pipeline import EntityRuler\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# see what we get before we do our stuff\n",
        "doc = nlp(\"Saxon Ground, St Aubyn's Road and A2040 are places, and Richard Burton is not\")\n",
        "displacy.render(doc, jupyter=True, style='ent')\n",
        "\n",
        "# make sure we load and run our custom pipeline before the standard one\n",
        "# else our entities get overwritten\n",
        "ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
        "\n",
        "# make a set of patterns from the csv\n",
        "# and then add them to our custom pipeline\n",
        "# note this relies on output_singlecolumn.csv being loaded into the notebook session\n",
        "# where output_singlecolumn.csv is extracted from \n",
        "# https://bitbucket.org/elenarobu/osopennames-geonames-columnsmap/src/main/\n",
        "opennames = pd.read_csv('output_singlecolumn.csv', dtype='string')\n",
        "opennames_list= opennames.iloc[:,0].to_list()\n",
        "patterns=[]\n",
        "for t in opennames_list:\n",
        "  patterns.append({\"label\":\"GPE\", \"pattern\":t})\n",
        "print(len(patterns))\n",
        "ruler.add_patterns(patterns)\n",
        "\n",
        "# test with a doc that has some entries from the list\n",
        "# and some that are not\n",
        "# and use displacy to render the text showing the entities\n",
        "doc = nlp(\"Saxon Ground, St Aubyn's Road and A2040 are places, and Richard Burton is not\")\n",
        "displacy.render(doc, jupyter=True, style='ent')\n"
      ],
      "metadata": {
        "id": "Nf07h2-IzOmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CF with another approach- phrase matching"
      ],
      "metadata": {
        "id": "0lnLdRWNpkVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "from spacy import displacy\n",
        "from spacy.matcher import PhraseMatcher\n",
        "\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "\n",
        "# make a set of patterns from the csv\n",
        "# and then create a phrase matcher as per \n",
        "# https://notebook.community/psychemedia/parlihacks/notebooks/Text%20Scraping%20-%20Notes\n",
        "# note this relies on output_singlecolumn.csv being loaded into the notebook session\n",
        "# where output_singlecolumn.csv is extracted from \n",
        "# https://bitbucket.org/elenarobu/osopennames-geonames-columnsmap/src/main/\n",
        "opennames = pd.read_csv('output_singlecolumn.csv', dtype='string')\n",
        "opennames_list= opennames.iloc[:,0].to_list()\n",
        "patterns=[]\n",
        "for t in opennames_list:\n",
        "  patterns.append({\"label\":\"GPE\", \"pattern\":t})\n",
        "print(len(patterns))\n",
        "\n",
        "patterns = [nlp(t) for t in opennames_list]\n",
        "matcher.add('GPE', patterns)\n",
        "\n",
        "\n",
        "# test with a doc that has some entries from the list\n",
        "# and some that are not\n",
        "doc = nlp(\"Saxon Ground, St Aubyn's Road and A2040 are places, and Richard Burton is not\")\n",
        "matches = matcher(doc)\n",
        "displacy.render(doc, jupyter=True, style='ent')"
      ],
      "metadata": {
        "id": "AJ9PFYEJomkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we need to test this out with the whole csv, which has now been committed to the github repository at https://github.com/archaeogeek/colab, so let's download it, check which folder we're in and that it has the right sort of data in it"
      ],
      "metadata": {
        "id": "YJ1b0It0dREF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/archaeogeek/colab/master/osopennames_singlecolumn_2.csv\n",
        "!pwd\n",
        "!head -n 10 osopennames_singlecolumn_2.csv"
      ],
      "metadata": {
        "id": "DFLl1PZudQcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the new model to disk"
      ],
      "metadata": {
        "id": "v2NLihYzgmTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "from spacy import displacy\n",
        "from spacy.pipeline import EntityRuler\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\", enable=[\"ner\"])\n",
        "# make sure we load and run our custom pipeline before the standard one\n",
        "# else our entities get overwritten\n",
        "ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
        "\n",
        "patterns=[]\n",
        "osopennames = pd.read_csv('output_singlecolumn.csv', dtype='string')\n",
        "o_list = osopennames.iloc[:,0]                        \n",
        "for t in o_list:\n",
        "  patterns.append({\"label\":\"GPE\", \"pattern\":t})                     \n",
        "print(len(patterns))\n",
        "ruler.add_patterns(patterns)\n",
        "nlp.to_disk(\"osopennames_ner\")\n",
        "\n",
        "# test with a doc that has some entries from the list\n",
        "# and some that are not\n",
        "# and use displacy to render the text showing the entities\n",
        "doc = nlp(\"Gloup is in my csv along with Westing. ZE2 9EH is not along with Saxon Ground, Doos' Geo and A2040. Los Angeles is not, and Richard Burton is certainly not!\")\n",
        "displacy.render(doc, jupyter=True, style='ent')\n",
        "\n"
      ],
      "metadata": {
        "id": "YKoLiETOJH9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading our custom model and actually using it. We're almost there but have to figure out how to re-add the `en_core_web_sm` model *and* our custom pipeline"
      ],
      "metadata": {
        "id": "DskPuRkhgrZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "nlp = spacy.load(\"osopennames_ner\")\n",
        "print(nlp.pipe_names)\n",
        "doc = nlp(\"I live at Greaves Road in Lancaster, Elena lives in Horndean down south somewhere. Mum lives in Endmoor, near Burton-in-Kendal\")\n",
        "displacy.render(doc, jupyter=True, style='ent')"
      ],
      "metadata": {
        "id": "xTnQJDlDdnas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mounting google drive to save model to- note need to force encoding to utf8. Note also that folder must already exist for copy command to work"
      ],
      "metadata": {
        "id": "SRbTFGBLeyMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n"
      ],
      "metadata": {
        "id": "DmzuDaPncIkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r \"/content/osopennames_ner\" \"/content/drive/MyDrive/osopennames_ner\""
      ],
      "metadata": {
        "id": "HM6qKMSxcpds"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}