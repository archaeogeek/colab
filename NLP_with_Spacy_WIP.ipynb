{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "F3zR3ZSU5ce4",
        "2rfLHUNwCG8a",
        "m7cZc9N0NF0R",
        "LLHPWN_y3uhp"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/archaeogeek/colab/blob/master/NLP_with_Spacy_WIP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tnrBNLGeu-f"
      },
      "source": [
        "Setting up working environment.\n",
        "Book says spaCy 2.0 or later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLVEgy6hekiy",
        "outputId": "a57c7802-88b0-47be-8890-9c359d42806b"
      },
      "source": [
        "!pip install spacy\n",
        "!pip install pytextrank --upgrade\n",
        "!python -m spacy info"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.4.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.25.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.6)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.4)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.11)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytextrank in /usr/local/lib/python3.8/dist-packages (3.2.4)\n",
            "Requirement already satisfied: icecream>=2.1 in /usr/local/lib/python3.8/dist-packages (from pytextrank) (2.1.3)\n",
            "Requirement already satisfied: networkx[default]>=2.6 in /usr/local/lib/python3.8/dist-packages (from pytextrank) (3.0)\n",
            "Requirement already satisfied: pygments>=2.7.4 in /usr/local/lib/python3.8/dist-packages (from pytextrank) (2.14.0)\n",
            "Requirement already satisfied: spacy>=3.0 in /usr/local/lib/python3.8/dist-packages (from pytextrank) (3.4.4)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.8/dist-packages (from pytextrank) (1.10.0)\n",
            "Requirement already satisfied: graphviz>=0.13 in /usr/local/lib/python3.8/dist-packages (from pytextrank) (0.20.1)\n",
            "Requirement already satisfied: executing>=0.3.1 in /usr/local/lib/python3.8/dist-packages (from icecream>=2.1->pytextrank) (1.2.0)\n",
            "Requirement already satisfied: asttokens>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from icecream>=2.1->pytextrank) (2.2.1)\n",
            "Requirement already satisfied: colorama>=0.3.9 in /usr/local/lib/python3.8/dist-packages (from icecream>=2.1->pytextrank) (0.4.6)\n",
            "Requirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.8/dist-packages (from networkx[default]>=2.6->pytextrank) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from networkx[default]>=2.6->pytextrank) (1.21.6)\n",
            "Requirement already satisfied: matplotlib>=3.4 in /usr/local/lib/python3.8/dist-packages (from networkx[default]>=2.6->pytextrank) (3.6.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0->pytextrank) (2.0.7)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0->pytextrank) (0.7.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0->pytextrank) (6.3.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0->pytextrank) (8.1.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0->pytextrank) (3.3.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0->pytextrank) (3.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0->pytextrank) (1.0.9)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0->pytextrank) (2.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0->pytextrank) (2.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0->pytextrank) (21.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0->pytextrank) (57.4.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0->pytextrank) (3.0.11)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0->pytextrank) (0.10.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0->pytextrank) (4.64.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0->pytextrank) (1.0.4)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0->pytextrank) (2.4.5)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0->pytextrank) (1.10.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0->pytextrank) (2.11.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0->pytextrank) (0.10.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from asttokens>=2.0.1->icecream>=2.1->pytextrank) (1.15.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.4->networkx[default]>=2.6->pytextrank) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.4->networkx[default]>=2.6->pytextrank) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.4->networkx[default]>=2.6->pytextrank) (2.8.2)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.4->networkx[default]>=2.6->pytextrank) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.4->networkx[default]>=2.6->pytextrank) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.4->networkx[default]>=2.6->pytextrank) (7.1.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.4->networkx[default]>=2.6->pytextrank) (4.38.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.3->networkx[default]>=2.6->pytextrank) (2022.7)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy>=3.0->pytextrank) (4.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (2.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.0->pytextrank) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.0->pytextrank) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy>=3.0->pytextrank) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy>=3.0->pytextrank) (2.0.1)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2023-01-24 18:05:52.654728: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\u001b[1m\n",
            "============================== Info about spaCy ==============================\u001b[0m\n",
            "\n",
            "spaCy version    3.4.4                         \n",
            "Location         /usr/local/lib/python3.8/dist-packages/spacy\n",
            "Platform         Linux-5.10.147+-x86_64-with-glibc2.29\n",
            "Python version   3.8.10                        \n",
            "Pipelines        en_core_web_sm (3.4.1)        \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpYeplT6fT3x"
      },
      "source": [
        "Install statistical models for spaCy. Models have naming syntax lang_type_genre_size. Downloading 'en' downloads the default en_core_web_md, which is the smallest general purpose model for the English language"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqkszNmyfTJ9",
        "outputId": "f4960826-65f0-4737-f34c-c2386f285932"
      },
      "source": [
        "!python -m spacy download en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2023-01-19 11:42:25.620464: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
            "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-sm==3.4.1) (3.4.4)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.4)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.25.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.11)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.4)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.0.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SkGV10IgK3j"
      },
      "source": [
        "# Basic NLP operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hj9kFoxfgTiy"
      },
      "source": [
        "**Tokenisation**\n",
        "Parsing text into tokens, which can be words, numbers or punctuation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p73NpzUEjIqy",
        "outputId": "35cd9a32-918b-4a66-c120-a960f6b6c997"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u'I am driving to Kendal')\n",
        "print([w.text for w in doc])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'driving', 'to', 'Kendal']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdlTcPUOlggM"
      },
      "source": [
        "**Lemmatization**\n",
        "The base form of any given token. eg, the Lemma for 'driving' is 'drive'. Should also work for tenses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Awh2MjxVlo00",
        "outputId": "be8ff947-5c0e-4053-a305-f29ef039ebd4"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u'this dog enjoyed both running and sleeping, in fact it slept very well and runs most days')\n",
        "for token in doc:\n",
        "  print(token.text, token.lemma_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this this\n",
            "dog dog\n",
            "enjoyed enjoy\n",
            "both both\n",
            "running run\n",
            "and and\n",
            "sleeping sleep\n",
            ", ,\n",
            "in in\n",
            "fact fact\n",
            "it it\n",
            "slept sleep\n",
            "very very\n",
            "well well\n",
            "and and\n",
            "runs run\n",
            "most most\n",
            "days day\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eQjU5CQp-2u"
      },
      "source": [
        "We need to deal with nicknames somehow- we can define them for spaCy to use. It doesn't seem to work if you just call 'doc' in the final line though- you seem to need to call it in the print statement. Maybe you need to define it after you add the tokenizer special case?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "ZO8eUyf1qGrd",
        "outputId": "69c2a4b7-a210-4075-ac4b-f79028c2bbc5"
      },
      "source": [
        "import spacy\n",
        "from spacy.symbols import ORTH, LEMMA\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u'I am driving to Lancaster, in Lancs')\n",
        "print([w.text for w in doc])\n",
        "special_case = [{ORTH: u'Lancs', LEMMA: u'Lancashire'}]\n",
        "nlp.tokenizer.add_special_case(u'Lancs', special_case)\n",
        "print([w.lemma_ for w in nlp(u'I am driving to Lancaster in Lancs')])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'driving', 'to', 'Lancaster', ',', 'in', 'Lancs']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-73768f5fa74c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mspecial_case\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mORTH\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mu'Lancs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEMMA\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mu'Lancashire'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_special_case\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'Lancs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecial_case\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'I am driving to Lancaster in Lancs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/spacy/tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.add_special_case\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/spacy/tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._validate_special_case\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [E1005] Unable to set attribute 'LEMMA' in tokenizer exception for 'Lancs'. Tokenizer exceptions are only allowed to specify ORTH and NORM."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVyvIS_StHB1"
      },
      "source": [
        "**Part-of-speech Tagging**\n",
        "Tags each word in a sentence with whether it's a noun, verb etc. Standard grammatical constructs, along with punctuation are called coarse-grained parts of speech. Tenses and types of pronoun etc are fine-grained parts of speech. NLP needs to do this for multi-sentence phrases as lemmatization will reduce everything to it's basic form- tense and anything signifying intent will disappear. The following snippet identifies the token that is in the *present progressive* form (`VBG`) or *base* form (`VB`) to identify the part of the phrase signifying intent rather than something that happened in the past."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixjB6lisLYMs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a0106eb-20ff-4dd4-bb31-89b37a5f5c3c"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u'I have driven to Lancaster. Now I am driving to Kendal')\n",
        "print([w.text for w in doc if w.tag_ == 'VBG' or w.tag_ == 'VB'])\n",
        "print([w.text for w in doc if w.pos_ == 'PROPN'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['driving']\n",
            "['Lancaster', 'Kendal']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNgCml2bOy0u"
      },
      "source": [
        "**Syntactic Relations** The next step is to link the proper noun with verb that best signifies intent. SpaCy uses syntactic dependency labels to show the relationship between pairs of words in a sentence. Verb phrases are parents, noun phrases are children (but can have children of their own). The relationship between parents or `heads`, and children is one-to-many, so the dependency label (`dobj`) is always assigned to the child\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hL8qgDHYRGGg",
        "outputId": "e0b7159c-ea06-47b4-abf6-80c04c9ecfa2"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u'I have driven to Lancaster. Now I am walking to Liverpool')\n",
        "for token in doc:\n",
        "  print(token.head.text, token.text, token.pos_, token.dep_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "driven I PRON nsubj\n",
            "driven have AUX aux\n",
            "driven driven VERB ROOT\n",
            "driven to ADP prep\n",
            "to Lancaster PROPN pobj\n",
            "driven . PUNCT punct\n",
            "walking Now ADV advmod\n",
            "walking I PRON nsubj\n",
            "walking am AUX aux\n",
            "walking walking VERB ROOT\n",
            "walking to ADP prep\n",
            "to Liverpool PROPN pobj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiobE5UySiFm"
      },
      "source": [
        "In the above, looking at the `token.head.text` and the `token.dep_` you can extract the `ROOT` and the `pobj` to extract the intent part of the phrase. We can also split the phrase up into sentences with `doc.sents`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtrMpWfCS9Ic",
        "outputId": "2a0f98e6-3986-4eae-d0d5-2fd4aab08231"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u'I have driven to Lancaster. Now I am cycling to Liverpool')\n",
        "for sent in doc.sents:\n",
        "  print([w.text for w in sent if w.dep_ == 'ROOT' or w.dep_ == 'pobj'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['driven', 'Lancaster']\n",
            "['cycling', 'Liverpool']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgVhFQAsTsBr"
      },
      "source": [
        "**Named Entity Recognition** named entities are real objects such as people, organisations, locations. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1KfD8AIT4GR",
        "outputId": "5cb117f1-e249-4ffe-d384-871c5e5a3308"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u'I have driven to Lancaster. Now I am walking to Galgate. I have been to Burton-in-Kendal too.')\n",
        "for token in doc:\n",
        "  if token.ent_type !=0:\n",
        "    print(token.text, token.ent_type_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lancaster GPE\n",
            "Galgate ORG\n",
            "Burton ORG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25de7053-7a06-43d0-de90-034e9161164e",
        "id": "EcduUykx4b01"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u'I have driven to Lancaster. Now I am walking to Galgate. I have been to Horndean and Burton-in-Kendal too.')\n",
        "for token in doc:\n",
        "  if token.ent_type !=0:\n",
        "    print(token.text, token.ent_type_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lancaster GPE\n",
            "Galgate ORG\n",
            "Horndean LOC\n",
            "Burton PERSON\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EDav6ZzN9m6"
      },
      "source": [
        "# Gotchas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MUCLesVOK5j"
      },
      "source": [
        "**UK Placenames** If we try a small village with a multi-part name such as *Burton-in-Kendal*, spaCy doesn't recognise it as a single proper noun. So we're going to need to try different entity taggers or dictionaries to find one that can understand more UK places (could be hills, mountains, etc too). If it's a single small place such as *Galgate* that's not in the list, it recognises it as a proper noun (`PROPN`) but doesn't tag it as the object (`pobj`). Then... if we use named entity recognition, it mis-tags *Lancaster* and *Burton* as `PERSON`, which is funny but problematic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a50zMviYWBqx"
      },
      "source": [
        "This might be the point at which we need to use something like [mordecai](https://github.com/openeventdata/mordecai) to do our geoparsing, which will require an ES install..."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternative approach- based on Chapter 10 (Training Models), there is an example that corrects the tag for a location from LOC to GPE. The section \"Automating the Example Creation Process\" takes a text file and a list of entities, and says \"for each entity in this list that appears in the text file, classify as GPE\". We should be able to both the text file and the list from the OS Open Names data and then use that to train the model. Let's see if we can use this approach to add Lancaster as a GPE in the model though..."
      ],
      "metadata": {
        "id": "mPMhN2p8zyhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.lang.char_classes import ALPHA, ALPHA_LOWER, ALPHA_UPPER\n",
        "from spacy.lang.char_classes import CONCAT_QUOTES, LIST_ELLIPSES, LIST_ICONS\n",
        "from spacy.util import compile_infix_regex\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Modify tokenizer infix patterns to stop the tokeniser breaking on hyphens\n",
        "# from https://spacy.io/usage/linguistic-features#native-tokenizers\n",
        "infixes = (\n",
        "    LIST_ELLIPSES\n",
        "    + LIST_ICONS\n",
        "    + [\n",
        "        r\"(?<=[0-9])[+\\\\-\\\\*^](?=[0-9-])\",\n",
        "        r\"(?<=[{al}{q}])\\\\.(?=[{au}{q}])\".format(\n",
        "            al=ALPHA_LOWER, au=ALPHA_UPPER, q=CONCAT_QUOTES\n",
        "        ),\n",
        "        r\"(?<=[{a}]),(?=[{a}])\".format(a=ALPHA),\n",
        "        # ✅ Commented out regex that splits on hyphens between letters:\n",
        "        # r\"(?<=[{a}])(?:{h})(?=[{a}])\".format(a=ALPHA, h=HYPHENS),\n",
        "        r\"(?<=[{a}0-9])[:<>=/](?=[{a}])\".format(a=ALPHA),\n",
        "    ]\n",
        ")\n",
        "\n",
        "infix_re = compile_infix_regex(infixes)\n",
        "nlp.tokenizer.infix_finditer = infix_re.finditer\n",
        "\n",
        "doc = nlp(u'Could you send a taxi to Horndean? I need to get to Lancaster, via Burton-in-Kendal, to meet Richard Burton')\n",
        "train_exams = []\n",
        "places = ['Horndean', 'Burton-in-Kendal', 'Lancaster']\n",
        "for sent in doc.sents:\n",
        "  entities = []\n",
        "  for token in sent:\n",
        "    # sanity check the existing token type and type number before we process\n",
        "    print(token.text, token.ent_type_, token.ent_type)\n",
        "    if token.text in places:\n",
        "      start = token.idx - sent.start_char\n",
        "      # force entries from the list to have an entity type of GPE\n",
        "      # note this doesn't affect the type number which could still be zero\n",
        "      entity = (start, start + len(token), 'GPE')\n",
        "    else:\n",
        "      start = token.idx - sent.start_char\n",
        "      # other entries not in the places list get their existing type and type number\n",
        "      entity = (start, start + len(token), token.ent_type_)\n",
        "    # prevent adding all the entities that still have no entity type\n",
        "    if entity[2] !='':\n",
        "      entities.append(entity)\n",
        "  tpl = (sent.text, {'entities': entities})\n",
        "  train_exams.append(tpl)\n",
        "# print resulting correctly tagged training examples\n",
        "print(train_exams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRdJv6a33JJD",
        "outputId": "bb897f36-0ae7-48ab-f980-977219bc6bf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could  0\n",
            "you  0\n",
            "send  0\n",
            "a  0\n",
            "taxi  0\n",
            "to  0\n",
            "Horndean LOC 385\n",
            "?  0\n",
            "I  0\n",
            "need  0\n",
            "to  0\n",
            "get  0\n",
            "to  0\n",
            "Lancaster PERSON 380\n",
            ",  0\n",
            "via  0\n",
            "Burton-in-Kendal  0\n",
            ",  0\n",
            "to  0\n",
            "meet  0\n",
            "Richard PERSON 380\n",
            "Burton PERSON 380\n",
            "[('Could you send a taxi to Horndean?', {'entities': [(25, 33, 'GPE')]}), ('I need to get to Lancaster, via Burton-in-Kendal, to meet Richard Burton', {'entities': [(17, 26, 'GPE'), (32, 48, 'GPE'), (58, 65, 'PERSON'), (66, 72, 'PERSON')]})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above approach seems to correctly tag the places and doesn't add all the other scrote to the list of training examples"
      ],
      "metadata": {
        "id": "cGM4vN8Q6IK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spacy Tokenisation\n"
      ],
      "metadata": {
        "id": "QNxmw3FjuxHc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aka how we deal with things like \"London Road\" without splitting it into \"London\" and \"Road\""
      ],
      "metadata": {
        "id": "U2JYsx0pu4SB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The example below shows using `token.rights` and `token.lefts` to pick up the left and right children of a token"
      ],
      "metadata": {
        "id": "tnjaE6MExtfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"I live on Greaves Road in Lancaster but I used to live on Greenfield Street\") #\"bright red apples on the tree\"\n",
        "for sent in doc.sents:\n",
        "  for token in sent:\n",
        "    print(token.text, token.ent_type_, [t.text for t in token.lefts], token.n_lefts, [t.text for t in token.rights], token.n_rights)\n",
        "# print([token.text for token in doc[2].lefts])  # ['bright', 'red']\n",
        "# print([token.text for token in doc[3].rights])  # ['on']\n",
        "print(doc[2].n_lefts)  # 2\n",
        "print(doc[2].n_rights)  # 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXnqFO7vvA02",
        "outputId": "805a8f13-dc9c-4375-af67-93788d37c2dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I  [] 0 [] 0\n",
            "live  ['I'] 1 ['on', 'but', 'used'] 3\n",
            "on  [] 0 ['Road'] 1\n",
            "Greaves FAC [] 0 [] 0\n",
            "Road FAC ['Greaves'] 1 ['in'] 1\n",
            "in  [] 0 ['Lancaster'] 1\n",
            "Lancaster GPE [] 0 [] 0\n",
            "but  [] 0 [] 0\n",
            "I  [] 0 [] 0\n",
            "used  ['I'] 1 ['live'] 1\n",
            "to  [] 0 [] 0\n",
            "live  ['to'] 1 ['on'] 1\n",
            "on  [] 0 ['Street'] 1\n",
            "Greenfield FAC [] 0 [] 0\n",
            "Street FAC ['Greenfield'] 1 [] 0\n",
            "0\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://stackoverflow.com/questions/57206701/spacy-tokenizer-rule-for-exceptions-that-contain-whitespace suggests a couple of approaches. The simplest might be to merge noun chunks?"
      ],
      "metadata": {
        "id": "-WYGwYCWIuhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp.add_pipe(\"merge_noun_chunks\")\n",
        "doc = nlp(u'I live on Greaves Road')\n",
        "for token in doc:\n",
        "  print(token.text, token.pos_, token.dep_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_BC1mBQJ0PY",
        "outputId": "e674b624-a849-4312-a250-25c2f0abb2cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I PRON nsubj\n",
            "live VERB ROOT\n",
            "on ADP prep\n",
            "Greaves Road PROPN pobj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding a terminology list to our model"
      ],
      "metadata": {
        "id": "QhKnsea2y_pK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thanks to https://notebook.community/psychemedia/parlihacks/notebooks/Text%20Scraping%20-%20Notes this might finally be the approach for pre-loading our additional location entries! The next step is (I think) to turn this into a custom pipeline that we can load alongside the proper one rather than doing the list munging each time"
      ],
      "metadata": {
        "id": "IJD2KxcAzGHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "from spacy import displacy\n",
        "from spacy.pipeline import EntityRuler\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# make sure we load and run our custom pipeline before the standard one\n",
        "# else our entities get overwritten\n",
        "ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
        "\n",
        "# make a set of patterns from the csv\n",
        "# and then add them to our custom pipeline\n",
        "# note this relies on top50_list.csv being loaded into the notebook session\n",
        "#opennames = pd.read_csv('top50_list.csv')\n",
        "opennames = pd.read_csv('osopennames_singlecolumn.csv')\n",
        "#opennames_list= opennames[\"THING\"].tolist()\n",
        "opennames_list= opennames[\"Entity_Name\"].tolist()\n",
        "patterns=[]\n",
        "for t in opennames_list:\n",
        "  patterns.append({\"label\":\"GPE\", \"pattern\":t})\n",
        "ruler.add_patterns(patterns)\n",
        "\n",
        "# test with a doc that has some entries from the list\n",
        "# and some that are not\n",
        "# and use displacy to render the text showing the entities\n",
        "doc = nlp(\"BN21 1AA is in my csv along with Saxon Ground, St Aubyn's Road and A2040. London is not, and Richard Burton is certainly not!\")\n",
        "displacy.render(doc, jupyter=True, style='ent')\n"
      ],
      "metadata": {
        "id": "Nf07h2-IzOmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we need to test this out with the whole csv, which has now been committed to the github repository at https://github.com/archaeogeek/colab, so let's download it, check which folder we're in and that it has the right sort of data in it"
      ],
      "metadata": {
        "id": "YJ1b0It0dREF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/archaeogeek/colab/master/osopennames_singlecolumn_2.csv\n",
        "!pwd\n",
        "!head -n 10 osopennames_singlecolumn_2.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFLl1PZudQcs",
        "outputId": "912dd001-0af5-43d8-cfc9-2cef094c2c89"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-08 20:36:58--  https://raw.githubusercontent.com/archaeogeek/colab/master/osopennames_singlecolumn_2.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 44453616 (42M) [text/plain]\n",
            "Saving to: ‘osopennames_singlecolumn_2.csv’\n",
            "\n",
            "osopennames_singlec 100%[===================>]  42.39M   166MB/s    in 0.3s    \n",
            "\n",
            "2023-02-08 20:36:58 (166 MB/s) - ‘osopennames_singlecolumn_2.csv’ saved [44453616/44453616]\n",
            "\n",
            "/content\n",
            "\"label\",\"pattern\"\n",
            "GPE,Westing\n",
            "GPE,Underhoull\n",
            "GPE,Gunnister\n",
            "GPE,Gloup\n",
            "GPE,Midbrake\n",
            "GPE,Uyeasound\n",
            "GPE,Burragarth\n",
            "GPE,Cullivoe\n",
            "GPE,Wick\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('osopennames_singlecolumn_2.csv')\n",
        "df[\"pattern\"] = df.pattern.apply(lambda x: {\"lower\": x})\n",
        "df.to_json(r'osopennames.jsonl', orient='records', lines=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "ES_6ZXNnvxuN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hmm, the below doesn't work :-("
      ],
      "metadata": {
        "id": "wpo755QS6HB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "from spacy import displacy\n",
        "from spacy.pipeline import EntityRuler\n",
        "\n",
        "config = {\n",
        "   \"path\": \"osopennames.jsonl\"\n",
        "}\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# make sure we load and run our custom pipeline before the standard one\n",
        "# else our entities get overwritten\n",
        "ruler = nlp.add_pipe(\"entity_ruler\")\n",
        "ruler.from_disk(\"osopennames.jsonl\")\n",
        "\n",
        "# test with a doc that has some entries from the list\n",
        "# and some that are not\n",
        "# and use displacy to render the text showing the entities\n",
        "doc = nlp(\"BN21 1AA is in my csv along with Saxon Ground, St Aubyn's Road and A2040. London is not, and Richard Burton is certainly not!\")\n",
        "displacy.render(doc, jupyter=True, style='ent')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "t79WCgI5zPnX",
        "outputId": "688951ed-e2a1-465e-fac6-6ff5d9243ccd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/pipeline/entityruler.py:390: UserWarning: [W036] The component 'entity_ruler' does not have any patterns defined.\n",
            "  warnings.warn(Warnings.W036.format(name=self.name))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">BN21 \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    1AA\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              " is in my csv along with \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Saxon Ground\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    St Aubyn's\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " Road and A2040. \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    London\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " is not, and \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Richard Burton\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " is certainly not!</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A side look at Geograpy #DEPRECATED\n"
      ],
      "metadata": {
        "id": "F3zR3ZSU5ce4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initial tests show that the location information is pretty basic, but it does get it from an sqllite db that perhaps we can tweak"
      ],
      "metadata": {
        "id": "VTj6HqGVx9lh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install geograpy3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H034hbAh5lFI",
        "outputId": "f6170a06-5a98-4b5d-8376-378c0cce9631"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: geograpy3 in /usr/local/lib/python3.8/dist-packages (0.2.5)\n",
            "Requirement already satisfied: nltk>=3.7 in /usr/local/lib/python3.8/dist-packages (from geograpy3) (3.7)\n",
            "Requirement already satisfied: pylodstorage~=0.4.7 in /usr/local/lib/python3.8/dist-packages (from geograpy3) (0.4.9)\n",
            "Requirement already satisfied: numpy>=1.21.4 in /usr/local/lib/python3.8/dist-packages (from geograpy3) (1.21.6)\n",
            "Requirement already satisfied: jellyfish>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from geograpy3) (0.9.0)\n",
            "Requirement already satisfied: newspaper3k>=0.2.8 in /usr/local/lib/python3.8/dist-packages (from geograpy3) (0.2.8)\n",
            "Requirement already satisfied: sphinx-rtd-theme in /usr/local/lib/python3.8/dist-packages (from geograpy3) (1.1.1)\n",
            "Requirement already satisfied: pandas>=1.3.5 in /usr/local/lib/python3.8/dist-packages (from geograpy3) (1.3.5)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.8/dist-packages (from geograpy3) (1.0.2)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.8/dist-packages (from newspaper3k>=0.2.8->geograpy3) (1.2.0)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.8/dist-packages (from newspaper3k>=0.2.8->geograpy3) (0.35.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.8/dist-packages (from newspaper3k>=0.2.8->geograpy3) (2.8.2)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.8/dist-packages (from newspaper3k>=0.2.8->geograpy3) (4.6.3)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from newspaper3k>=0.2.8->geograpy3) (6.0.10)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.8/dist-packages (from newspaper3k>=0.2.8->geograpy3) (0.3)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.8/dist-packages (from newspaper3k>=0.2.8->geograpy3) (7.1.2)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from newspaper3k>=0.2.8->geograpy3) (3.4.0)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from newspaper3k>=0.2.8->geograpy3) (0.0.4)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.8/dist-packages (from newspaper3k>=0.2.8->geograpy3) (2.25.1)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.8/dist-packages (from newspaper3k>=0.2.8->geograpy3) (6.0)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.8/dist-packages (from newspaper3k>=0.2.8->geograpy3) (4.9.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk>=3.7->geograpy3) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk>=3.7->geograpy3) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk>=3.7->geograpy3) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk>=3.7->geograpy3) (2022.6.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.3.5->geograpy3) (2022.7)\n",
            "Requirement already satisfied: SPARQLWrapper~=1.8.5 in /usr/local/lib/python3.8/dist-packages (from pylodstorage~=0.4.7->geograpy3) (1.8.5)\n",
            "Requirement already satisfied: pylatexenc~=2.10 in /usr/local/lib/python3.8/dist-packages (from pylodstorage~=0.4.7->geograpy3) (2.10)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from pylodstorage~=0.4.7->geograpy3) (0.8.10)\n",
            "Requirement already satisfied: jsonpickle==1.5.2 in /usr/local/lib/python3.8/dist-packages (from pylodstorage~=0.4.7->geograpy3) (1.5.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from pylodstorage~=0.4.7->geograpy3) (2.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from pylodstorage~=0.4.7->geograpy3) (3.2.2)\n",
            "Requirement already satisfied: dicttoxml2 in /usr/local/lib/python3.8/dist-packages (from pylodstorage~=0.4.7->geograpy3) (2.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.2->geograpy3) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.2->geograpy3) (3.1.0)\n",
            "Requirement already satisfied: sphinx<6,>=1.6 in /usr/local/lib/python3.8/dist-packages (from sphinx-rtd-theme->geograpy3) (3.5.4)\n",
            "Requirement already satisfied: docutils<0.18 in /usr/local/lib/python3.8/dist-packages (from sphinx-rtd-theme->geograpy3) (0.16)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from feedfinder2>=0.0.4->newspaper3k>=0.2.8->geograpy3) (1.15.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.8/dist-packages (from feedparser>=5.2.1->newspaper3k>=0.2.8->geograpy3) (1.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.10.0->newspaper3k>=0.2.8->geograpy3) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.10.0->newspaper3k>=0.2.8->geograpy3) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.10.0->newspaper3k>=0.2.8->geograpy3) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.10.0->newspaper3k>=0.2.8->geograpy3) (4.0.0)\n",
            "Requirement already satisfied: rdflib>=4.0 in /usr/local/lib/python3.8/dist-packages (from SPARQLWrapper~=1.8.5->pylodstorage~=0.4.7->geograpy3) (6.2.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp in /usr/local/lib/python3.8/dist-packages (from sphinx<6,>=1.6->sphinx-rtd-theme->geograpy3) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.8/dist-packages (from sphinx<6,>=1.6->sphinx-rtd-theme->geograpy3) (1.0.3)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.8/dist-packages (from sphinx<6,>=1.6->sphinx-rtd-theme->geograpy3) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.8/dist-packages (from sphinx<6,>=1.6->sphinx-rtd-theme->geograpy3) (1.0.2)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.8/dist-packages (from sphinx<6,>=1.6->sphinx-rtd-theme->geograpy3) (0.7.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from sphinx<6,>=1.6->sphinx-rtd-theme->geograpy3) (21.3)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.8/dist-packages (from sphinx<6,>=1.6->sphinx-rtd-theme->geograpy3) (2.2.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.8/dist-packages (from sphinx<6,>=1.6->sphinx-rtd-theme->geograpy3) (1.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from sphinx<6,>=1.6->sphinx-rtd-theme->geograpy3) (57.4.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.8/dist-packages (from sphinx<6,>=1.6->sphinx-rtd-theme->geograpy3) (2.11.0)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.8/dist-packages (from sphinx<6,>=1.6->sphinx-rtd-theme->geograpy3) (1.0.3)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.8/dist-packages (from sphinx<6,>=1.6->sphinx-rtd-theme->geograpy3) (1.4.1)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.8/dist-packages (from sphinx<6,>=1.6->sphinx-rtd-theme->geograpy3) (2.11.3)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.8/dist-packages (from tldextract>=2.0.1->newspaper3k>=0.2.8->geograpy3) (3.9.0)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.8/dist-packages (from tldextract>=2.0.1->newspaper3k>=0.2.8->geograpy3) (1.5.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->pylodstorage~=0.4.7->geograpy3) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->pylodstorage~=0.4.7->geograpy3) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->pylodstorage~=0.4.7->geograpy3) (0.11.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from Jinja2>=2.3->sphinx<6,>=1.6->sphinx-rtd-theme->geograpy3) (2.0.1)\n",
            "Requirement already satisfied: isodate in /usr/local/lib/python3.8/dist-packages (from rdflib>=4.0->SPARQLWrapper~=1.8.5->pylodstorage~=0.4.7->geograpy3) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import geograpy\n",
        "test = \"I am from Lancaster\"\n",
        "places = geograpy.get_geoPlace_context(text=test)\n",
        "print(places)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bojJ0FLt6KLW",
        "outputId": "b197216f-f999-4c0f-f33c-ba052848be7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "countries=[]\n",
            "regions=[]\n",
            "cities=[]\n",
            "other=[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A side look at Geopy #DEPRECATED"
      ],
      "metadata": {
        "id": "2rfLHUNwCG8a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Technically a geocoder rather than a geoparser- it copes very well with our standard examples, and even postcodes, but fails when the string contains more than just the location (eg \"I live at Greaves Road\")."
      ],
      "metadata": {
        "id": "W6apVpOSDVPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install geopy --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "_yoE4q1wCMVq",
        "outputId": "6f02f995-59f5-498d-e28b-343189751579"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: geopy in /usr/local/lib/python3.8/dist-packages (1.17.0)\n",
            "Collecting geopy\n",
            "  Downloading geopy-2.3.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 KB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: geographiclib<3,>=1.52 in /usr/local/lib/python3.8/dist-packages (from geopy) (1.52)\n",
            "Installing collected packages: geopy\n",
            "  Attempting uninstall: geopy\n",
            "    Found existing installation: geopy 1.17.0\n",
            "    Uninstalling geopy-1.17.0:\n",
            "      Successfully uninstalled geopy-1.17.0\n",
            "Successfully installed geopy-2.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "geopy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from geopy.geocoders import Nominatim\n",
        "geolocator = Nominatim(user_agent=\"geocode_test\")\n",
        "location = geolocator.geocode(\"LA1 4UR\", country_codes=\"gb\")\n",
        "print(location.address)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soiXeM-0CZzR",
        "outputId": "a631d5c1-de42-4027-b368-95b3fe51c117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lancaster, Lancashire, England, LA1 4UR, United Kingdom\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So... maybe the approach is to use Spacy for the tokenisation and then pass the tokens to geopy?\n",
        "We'd need to include:\n",
        "* regex for postcodes\n",
        "* regex for common street suffixes\n",
        "* hyphenisation"
      ],
      "metadata": {
        "id": "A3G29X15HehC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Putting it all together #DEPRECATED"
      ],
      "metadata": {
        "id": "m7cZc9N0NF0R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below combines the hyphenisation and merging noun chunks. It gives pretty good results *when* it properly classifies something as a noun but note that for Horndean it misclassifies it as an adjective. We still need some custom regex for dealing with postcodes"
      ],
      "metadata": {
        "id": "eQMrNsNnNIvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.lang.char_classes import ALPHA, ALPHA_LOWER, ALPHA_UPPER\n",
        "from spacy.lang.char_classes import CONCAT_QUOTES, LIST_ELLIPSES, LIST_ICONS\n",
        "from spacy.util import compile_infix_regex\n",
        "from geopy.geocoders import Nominatim\n",
        "geolocator = Nominatim(user_agent=\"geocode_test\")\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Modify tokenizer infix patterns to stop the tokeniser breaking on hyphens\n",
        "# from https://spacy.io/usage/linguistic-features#native-tokenizers\n",
        "infixes = (\n",
        "    LIST_ELLIPSES\n",
        "    + LIST_ICONS\n",
        "    + [\n",
        "        r\"(?<=[0-9])[+\\\\-\\\\*^](?=[0-9-])\",\n",
        "        r\"(?<=[{al}{q}])\\\\.(?=[{au}{q}])\".format(\n",
        "            al=ALPHA_LOWER, au=ALPHA_UPPER, q=CONCAT_QUOTES\n",
        "        ),\n",
        "        r\"(?<=[{a}]),(?=[{a}])\".format(a=ALPHA),\n",
        "        # ✅ Commented out regex that splits on hyphens between letters:\n",
        "        # r\"(?<=[{a}])(?:{h})(?=[{a}])\".format(a=ALPHA, h=HYPHENS),\n",
        "        r\"(?<=[{a}0-9])[:<>=/](?=[{a}])\".format(a=ALPHA),\n",
        "    ]\n",
        ")\n",
        "\n",
        "infix_re = compile_infix_regex(infixes)\n",
        "nlp.tokenizer.infix_finditer = infix_re.finditer\n",
        "\n",
        "# merge noun chunks into single tokens\n",
        "nlp.add_pipe(\"merge_noun_chunks\")\n",
        "doc = nlp(u'I am a dog called Richard Burton. I live on Jensen Close but used to live in Greenfield Street, before that I lived in Burton-in-Kendal and I want to live in Horndean')\n",
        "for token in doc:\n",
        "  print(token.text, token.pos_, token.dep_)\n",
        "  if token.pos_ == \"PROPN\":\n",
        "    location = geolocator.geocode(token,country_codes=\"gb\")\n",
        "    print(location.address)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lRXQ-FcKhJZ",
        "outputId": "a8774716-600c-4280-fe1d-3ba42b367ecf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I PRON nsubj\n",
            "am AUX ROOT\n",
            "a dog NOUN attr\n",
            "called VERB acl\n",
            "Richard Burton PROPN oprd\n",
            "Richard Burton, 6, Lyndhurst Road, Belsize Park, London Borough of Camden, London, Greater London, England, NW3 5PE, United Kingdom\n",
            ". PUNCT punct\n",
            "I PRON nsubj\n",
            "live VERB ROOT\n",
            "on ADP prep\n",
            "Jensen Close PROPN pobj\n",
            "Jensen Close, Haverbreaks, Aldcliffe, Lancaster, Lancashire, England, LA1 5BP, United Kingdom\n",
            "but CCONJ cc\n",
            "used VERB conj\n",
            "to PART aux\n",
            "live VERB xcomp\n",
            "in ADP prep\n",
            "Greenfield Street PROPN pobj\n",
            "Greenfield Street, Holywell, Flintshire, Cymru / Wales, CH8 7PN, United Kingdom\n",
            ", PUNCT punct\n",
            "before ADP prep\n",
            "that PRON pobj\n",
            "I PRON nsubj\n",
            "lived VERB advcl\n",
            "in ADP prep\n",
            "Burton-in-Kendal PROPN pobj\n",
            "Burton-in-Kendal, South Lakeland, Cumbria, England, United Kingdom\n",
            "and CCONJ cc\n",
            "I PRON nsubj\n",
            "want VERB conj\n",
            "to PART aux\n",
            "live VERB xcomp\n",
            "in ADP prep\n",
            "Horndean ADJ pobj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keyword extraction\n",
        "Examples include https://betterprogramming.pub/extract-keywords-using-spacy-in-python-4a8415478fbf which works but is a bit unsophisticated. Pytextrank ought to be great but I can't get the simple code from https://github.com/DerwenAI/pytextrank to run\n"
      ],
      "metadata": {
        "id": "LLHPWN_y3uhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pytextrank\n",
        "\n",
        "# example text\n",
        "text = \"Compatibility of systems of linear constraints over the set of natural numbers. Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given. These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types.\"\n",
        "\n",
        "# load a spaCy model, depending on language, scale, etc.\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# add PyTextRank to the spaCy pipeline\n",
        "nlp.add_pipe(\"textrank\")\n",
        "doc = nlp(text)\n",
        "\n",
        "# examine the top-ranked phrases in the document\n",
        "for phrase in doc._.phrases:\n",
        "    print(phrase.text)\n",
        "    print(phrase.rank, phrase.count)\n",
        "    print(phrase.chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "__9o0AST31Wl",
        "outputId": "a73f3e9c-bca7-4508-8056-baa70a1aead2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-2612aa8cbaa3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# add PyTextRank to the spaCy pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"textrank\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# examine the top-ranked phrases in the document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1029\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE109\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1031\u001b[0;31m                 \u001b[0merror_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1032\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturned_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mraise_error\u001b[0;34m(proc_name, proc, docs, e)\u001b[0m\n\u001b[1;32m   1668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1669\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1670\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1024\u001b[0m                 \u001b[0merror_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_error_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m                 \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m                 \u001b[0;31m# This typically happens if a component is not initialized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytextrank/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    251\u001b[0m             )\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphrases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextrank\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_textrank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytextrank/base.py\u001b[0m in \u001b[0;36mcalc_textrank\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;31m# for PageRank (i.e., based on eigenvector centrality)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;31m# to calculate a rank for each node in the lemma graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         self.ranks = nx.pagerank(\n\u001b[0m\u001b[1;32m    353\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0mpersonalization\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_personalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/networkx/classes/backends.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    143\u001b[0m                         \u001b[0;34mf\"'{name}' not implemented by {plugin_name}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                     )\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0m_register_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/networkx/algorithms/link_analysis/pagerank_alg.py\u001b[0m in \u001b[0;36mpagerank\u001b[0;34m(G, alpha, personalization, max_iter, tol, nstart, weight, dangling)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \"\"\"\n\u001b[0;32m--> 109\u001b[0;31m     return _pagerank_scipy(\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpersonalization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdangling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/networkx/algorithms/link_analysis/pagerank_alg.py\u001b[0m in \u001b[0;36m_pagerank_scipy\u001b[0;34m(G, alpha, personalization, max_iter, tol, nstart, weight, dangling)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[0mnodelist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_scipy_sparse_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodelist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnodelist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m     \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0mS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mS\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mS\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/networkx/convert_matrix.py\u001b[0m in \u001b[0;36mto_scipy_sparse_array\u001b[0;34m(G, nodelist, dtype, weight, format)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_directed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoo_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;31m# symmetrize matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'scipy.sparse' has no attribute 'coo_array'"
          ]
        }
      ]
    }
  ]
}