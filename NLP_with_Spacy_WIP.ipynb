{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/archaeogeek/colab/blob/main/NLP_with_Spacy_WIP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tnrBNLGeu-f"
      },
      "source": [
        "Setting up working environment.\n",
        "Book says spaCy 2.0 or later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLVEgy6hekiy",
        "outputId": "a57c7802-88b0-47be-8890-9c359d42806b"
      },
      "source": [
        "!pip install spacy\n",
        "!pip install pytextrank --upgrade\n",
        "!python -m spacy info"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.4.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.25.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.6)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.4)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.11)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytextrank in /usr/local/lib/python3.8/dist-packages (3.2.4)\n",
            "Requirement already satisfied: icecream>=2.1 in /usr/local/lib/python3.8/dist-packages (from pytextrank) (2.1.3)\n",
            "Requirement already satisfied: networkx[default]>=2.6 in /usr/local/lib/python3.8/dist-packages (from pytextrank) (3.0)\n",
            "Requirement already satisfied: pygments>=2.7.4 in /usr/local/lib/python3.8/dist-packages (from pytextrank) (2.14.0)\n",
            "Requirement already satisfied: spacy>=3.0 in /usr/local/lib/python3.8/dist-packages (from pytextrank) (3.4.4)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.8/dist-packages (from pytextrank) (1.10.0)\n",
            "Requirement already satisfied: graphviz>=0.13 in /usr/local/lib/python3.8/dist-packages (from pytextrank) (0.20.1)\n",
            "Requirement already satisfied: executing>=0.3.1 in /usr/local/lib/python3.8/dist-packages (from icecream>=2.1->pytextrank) (1.2.0)\n",
            "Requirement already satisfied: asttokens>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from icecream>=2.1->pytextrank) (2.2.1)\n",
            "Requirement already satisfied: colorama>=0.3.9 in /usr/local/lib/python3.8/dist-packages (from icecream>=2.1->pytextrank) (0.4.6)\n",
            "Requirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.8/dist-packages (from networkx[default]>=2.6->pytextrank) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from networkx[default]>=2.6->pytextrank) (1.21.6)\n",
            "Requirement already satisfied: matplotlib>=3.4 in /usr/local/lib/python3.8/dist-packages (from networkx[default]>=2.6->pytextrank) (3.6.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0->pytextrank) (2.0.7)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0->pytextrank) (0.7.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0->pytextrank) (6.3.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0->pytextrank) (8.1.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0->pytextrank) (3.3.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0->pytextrank) (3.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0->pytextrank) (1.0.9)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0->pytextrank) (2.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0->pytextrank) (2.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0->pytextrank) (21.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0->pytextrank) (57.4.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0->pytextrank) (3.0.11)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0->pytextrank) (0.10.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0->pytextrank) (4.64.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0->pytextrank) (1.0.4)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0->pytextrank) (2.4.5)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0->pytextrank) (1.10.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0->pytextrank) (2.11.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0->pytextrank) (0.10.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from asttokens>=2.0.1->icecream>=2.1->pytextrank) (1.15.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.4->networkx[default]>=2.6->pytextrank) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.4->networkx[default]>=2.6->pytextrank) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.4->networkx[default]>=2.6->pytextrank) (2.8.2)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.4->networkx[default]>=2.6->pytextrank) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.4->networkx[default]>=2.6->pytextrank) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.4->networkx[default]>=2.6->pytextrank) (7.1.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.4->networkx[default]>=2.6->pytextrank) (4.38.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.3->networkx[default]>=2.6->pytextrank) (2022.7)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy>=3.0->pytextrank) (4.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (2.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.0->pytextrank) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.0->pytextrank) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy>=3.0->pytextrank) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy>=3.0->pytextrank) (2.0.1)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2023-01-24 18:05:52.654728: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\u001b[1m\n",
            "============================== Info about spaCy ==============================\u001b[0m\n",
            "\n",
            "spaCy version    3.4.4                         \n",
            "Location         /usr/local/lib/python3.8/dist-packages/spacy\n",
            "Platform         Linux-5.10.147+-x86_64-with-glibc2.29\n",
            "Python version   3.8.10                        \n",
            "Pipelines        en_core_web_sm (3.4.1)        \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpYeplT6fT3x"
      },
      "source": [
        "Install statistical models for spaCy. Models have naming syntax lang_type_genre_size. Downloading 'en' downloads the default en_core_web_md, which is the smallest general purpose model for the English language"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqkszNmyfTJ9",
        "outputId": "f4960826-65f0-4737-f34c-c2386f285932"
      },
      "source": [
        "!python -m spacy download en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2023-01-19 11:42:25.620464: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
            "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-sm==3.4.1) (3.4.4)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.4)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.25.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.11)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.4)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.0.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SkGV10IgK3j"
      },
      "source": [
        "# Basic NLP operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hj9kFoxfgTiy"
      },
      "source": [
        "**Tokenisation**\n",
        "Parsing text into tokens, which can be words, numbers or punctuation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p73NpzUEjIqy",
        "outputId": "35cd9a32-918b-4a66-c120-a960f6b6c997"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u'I am driving to Kendal')\n",
        "print([w.text for w in doc])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'driving', 'to', 'Kendal']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdlTcPUOlggM"
      },
      "source": [
        "**Lemmatization**\n",
        "The base form of any given token. eg, the Lemma for 'driving' is 'drive'. Should also work for tenses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Awh2MjxVlo00",
        "outputId": "be8ff947-5c0e-4053-a305-f29ef039ebd4"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u'this dog enjoyed both running and sleeping, in fact it slept very well and runs most days')\n",
        "for token in doc:\n",
        "  print(token.text, token.lemma_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this this\n",
            "dog dog\n",
            "enjoyed enjoy\n",
            "both both\n",
            "running run\n",
            "and and\n",
            "sleeping sleep\n",
            ", ,\n",
            "in in\n",
            "fact fact\n",
            "it it\n",
            "slept sleep\n",
            "very very\n",
            "well well\n",
            "and and\n",
            "runs run\n",
            "most most\n",
            "days day\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eQjU5CQp-2u"
      },
      "source": [
        "We need to deal with nicknames somehow- we can define them for spaCy to use. It doesn't seem to work if you just call 'doc' in the final line though- you seem to need to call it in the print statement. Maybe you need to define it after you add the tokenizer special case?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "ZO8eUyf1qGrd",
        "outputId": "69c2a4b7-a210-4075-ac4b-f79028c2bbc5"
      },
      "source": [
        "import spacy\n",
        "from spacy.symbols import ORTH, LEMMA\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u'I am driving to Lancaster, in Lancs')\n",
        "print([w.text for w in doc])\n",
        "special_case = [{ORTH: u'Lancs', LEMMA: u'Lancashire'}]\n",
        "nlp.tokenizer.add_special_case(u'Lancs', special_case)\n",
        "print([w.lemma_ for w in nlp(u'I am driving to Lancaster in Lancs')])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'driving', 'to', 'Lancaster', ',', 'in', 'Lancs']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-73768f5fa74c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mspecial_case\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mORTH\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mu'Lancs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEMMA\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mu'Lancashire'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_special_case\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'Lancs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecial_case\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'I am driving to Lancaster in Lancs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/spacy/tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.add_special_case\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/spacy/tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._validate_special_case\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [E1005] Unable to set attribute 'LEMMA' in tokenizer exception for 'Lancs'. Tokenizer exceptions are only allowed to specify ORTH and NORM."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVyvIS_StHB1"
      },
      "source": [
        "**Part-of-speech Tagging**\n",
        "Tags each word in a sentence with whether it's a noun, verb etc. Standard grammatical constructs, along with punctuation are called coarse-grained parts of speech. Tenses and types of pronoun etc are fine-grained parts of speech. NLP needs to do this for multi-sentence phrases as lemmatization will reduce everything to it's basic form- tense and anything signifying intent will disappear. The following snippet identifies the token that is in the *present progressive* form (`VBG`) or *base* form (`VB`) to identify the part of the phrase signifying intent rather than something that happened in the past."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixjB6lisLYMs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a0106eb-20ff-4dd4-bb31-89b37a5f5c3c"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u'I have driven to Lancaster. Now I am driving to Kendal')\n",
        "print([w.text for w in doc if w.tag_ == 'VBG' or w.tag_ == 'VB'])\n",
        "print([w.text for w in doc if w.pos_ == 'PROPN'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['driving']\n",
            "['Lancaster', 'Kendal']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNgCml2bOy0u"
      },
      "source": [
        "**Syntactic Relations** The next step is to link the proper noun with verb that best signifies intent. SpaCy uses syntactic dependency labels to show the relationship between pairs of words in a sentence. Verb phrases are parents, noun phrases are children (but can have children of their own). The relationship between parents or `heads`, and children is one-to-many, so the dependency label (`dobj`) is always assigned to the child\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hL8qgDHYRGGg",
        "outputId": "e0b7159c-ea06-47b4-abf6-80c04c9ecfa2"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u'I have driven to Lancaster. Now I am walking to Liverpool')\n",
        "for token in doc:\n",
        "  print(token.head.text, token.text, token.pos_, token.dep_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "driven I PRON nsubj\n",
            "driven have AUX aux\n",
            "driven driven VERB ROOT\n",
            "driven to ADP prep\n",
            "to Lancaster PROPN pobj\n",
            "driven . PUNCT punct\n",
            "walking Now ADV advmod\n",
            "walking I PRON nsubj\n",
            "walking am AUX aux\n",
            "walking walking VERB ROOT\n",
            "walking to ADP prep\n",
            "to Liverpool PROPN pobj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiobE5UySiFm"
      },
      "source": [
        "In the above, looking at the `token.head.text` and the `token.dep_` you can extract the `ROOT` and the `pobj` to extract the intent part of the phrase. We can also split the phrase up into sentences with `doc.sents`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtrMpWfCS9Ic",
        "outputId": "2a0f98e6-3986-4eae-d0d5-2fd4aab08231"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u'I have driven to Lancaster. Now I am cycling to Liverpool')\n",
        "for sent in doc.sents:\n",
        "  print([w.text for w in sent if w.dep_ == 'ROOT' or w.dep_ == 'pobj'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['driven', 'Lancaster']\n",
            "['cycling', 'Liverpool']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgVhFQAsTsBr"
      },
      "source": [
        "**Named Entity Recognition** named entities are real objects such as people, organisations, locations. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1KfD8AIT4GR",
        "outputId": "5cb117f1-e249-4ffe-d384-871c5e5a3308"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u'I have driven to Lancaster. Now I am walking to Galgate. I have been to Burton-in-Kendal too.')\n",
        "for token in doc:\n",
        "  if token.ent_type !=0:\n",
        "    print(token.text, token.ent_type_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lancaster GPE\n",
            "Galgate ORG\n",
            "Burton ORG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25de7053-7a06-43d0-de90-034e9161164e",
        "id": "EcduUykx4b01"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u'I have driven to Lancaster. Now I am walking to Galgate. I have been to Horndean and Burton-in-Kendal too.')\n",
        "for token in doc:\n",
        "  if token.ent_type !=0:\n",
        "    print(token.text, token.ent_type_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lancaster GPE\n",
            "Galgate ORG\n",
            "Horndean LOC\n",
            "Burton PERSON\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EDav6ZzN9m6"
      },
      "source": [
        "# Gotchas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MUCLesVOK5j"
      },
      "source": [
        "**UK Placenames** If we try a small village with a multi-part name such as *Burton-in-Kendal*, spaCy doesn't recognise it as a single proper noun. So we're going to need to try different entity taggers or dictionaries to find one that can understand more UK places (could be hills, mountains, etc too). If it's a single small place such as *Galgate* that's not in the list, it recognises it as a proper noun (`PROPN`) but doesn't tag it as the object (`pobj`). Then... if we use named entity recognition, it mis-tags *Lancaster* and *Burton* as `PERSON`, which is funny but problematic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a50zMviYWBqx"
      },
      "source": [
        "This might be the point at which we need to use something like [mordecai](https://github.com/openeventdata/mordecai) to do our geoparsing, which will require an ES install..."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternative approach- based on Chapter 10 (Training Models), there is an example that corrects the tag for a location from LOC to GPE. The section \"Automating the Example Creation Process\" takes a text file and a list of entities, and says \"for each entity in this list that appears in the text file, classify as GPE\". We should be able to both the text file and the list from the OS Open Names data and then use that to train the model. Let's see if we can use this approach to add Lancaster as a GPE in the model though..."
      ],
      "metadata": {
        "id": "mPMhN2p8zyhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.lang.char_classes import ALPHA, ALPHA_LOWER, ALPHA_UPPER\n",
        "from spacy.lang.char_classes import CONCAT_QUOTES, LIST_ELLIPSES, LIST_ICONS\n",
        "from spacy.util import compile_infix_regex\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Modify tokenizer infix patterns to stop the tokeniser breaking on hyphens\n",
        "# from https://spacy.io/usage/linguistic-features#native-tokenizers\n",
        "infixes = (\n",
        "    LIST_ELLIPSES\n",
        "    + LIST_ICONS\n",
        "    + [\n",
        "        r\"(?<=[0-9])[+\\\\-\\\\*^](?=[0-9-])\",\n",
        "        r\"(?<=[{al}{q}])\\\\.(?=[{au}{q}])\".format(\n",
        "            al=ALPHA_LOWER, au=ALPHA_UPPER, q=CONCAT_QUOTES\n",
        "        ),\n",
        "        r\"(?<=[{a}]),(?=[{a}])\".format(a=ALPHA),\n",
        "        # ✅ Commented out regex that splits on hyphens between letters:\n",
        "        # r\"(?<=[{a}])(?:{h})(?=[{a}])\".format(a=ALPHA, h=HYPHENS),\n",
        "        r\"(?<=[{a}0-9])[:<>=/](?=[{a}])\".format(a=ALPHA),\n",
        "    ]\n",
        ")\n",
        "\n",
        "infix_re = compile_infix_regex(infixes)\n",
        "nlp.tokenizer.infix_finditer = infix_re.finditer\n",
        "\n",
        "doc = nlp(u'Could you send a taxi to Horndean? I need to get to Lancaster, via Burton-in-Kendal, to meet Richard Burton')\n",
        "train_exams = []\n",
        "places = ['Horndean', 'Burton-in-Kendal', 'Lancaster']\n",
        "for sent in doc.sents:\n",
        "  entities = []\n",
        "  for token in sent:\n",
        "    # sanity check the existing token type and type number before we process\n",
        "    print(token.text, token.ent_type_, token.ent_type)\n",
        "    if token.text in places:\n",
        "      start = token.idx - sent.start_char\n",
        "      # force entries from the list to have an entity type of GPE\n",
        "      # note this doesn't affect the type number which could still be zero\n",
        "      entity = (start, start + len(token), 'GPE')\n",
        "    else:\n",
        "      start = token.idx - sent.start_char\n",
        "      # other entries not in the places list get their existing type and type number\n",
        "      entity = (start, start + len(token), token.ent_type_)\n",
        "    # prevent adding all the entities that still have no entity type\n",
        "    if entity[2] !='':\n",
        "      entities.append(entity)\n",
        "  tpl = (sent.text, {'entities': entities})\n",
        "  train_exams.append(tpl)\n",
        "# print resulting correctly tagged training examples\n",
        "print(train_exams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRdJv6a33JJD",
        "outputId": "bb897f36-0ae7-48ab-f980-977219bc6bf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could  0\n",
            "you  0\n",
            "send  0\n",
            "a  0\n",
            "taxi  0\n",
            "to  0\n",
            "Horndean LOC 385\n",
            "?  0\n",
            "I  0\n",
            "need  0\n",
            "to  0\n",
            "get  0\n",
            "to  0\n",
            "Lancaster PERSON 380\n",
            ",  0\n",
            "via  0\n",
            "Burton-in-Kendal  0\n",
            ",  0\n",
            "to  0\n",
            "meet  0\n",
            "Richard PERSON 380\n",
            "Burton PERSON 380\n",
            "[('Could you send a taxi to Horndean?', {'entities': [(25, 33, 'GPE')]}), ('I need to get to Lancaster, via Burton-in-Kendal, to meet Richard Burton', {'entities': [(17, 26, 'GPE'), (32, 48, 'GPE'), (58, 65, 'PERSON'), (66, 72, 'PERSON')]})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above approach seems to correctly tag the places and doesn't add all the other scrote to the list of training examples"
      ],
      "metadata": {
        "id": "cGM4vN8Q6IK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spacy Tokenisation\n"
      ],
      "metadata": {
        "id": "QNxmw3FjuxHc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aka how we deal with things like \"London Road\" without splitting it into \"London\" and \"Road\""
      ],
      "metadata": {
        "id": "U2JYsx0pu4SB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The example below shows using `token.rights` and `token.lefts` to pick up the left and right children of a token"
      ],
      "metadata": {
        "id": "tnjaE6MExtfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"I live on Greaves Road in Lancaster but I used to live on Greenfield Street\") #\"bright red apples on the tree\"\n",
        "for sent in doc.sents:\n",
        "  for token in sent:\n",
        "    print(token.text, token.ent_type_, [t.text for t in token.lefts], token.n_lefts, [t.text for t in token.rights], token.n_rights)\n",
        "# print([token.text for token in doc[2].lefts])  # ['bright', 'red']\n",
        "# print([token.text for token in doc[3].rights])  # ['on']\n",
        "print(doc[2].n_lefts)  # 2\n",
        "print(doc[2].n_rights)  # 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXnqFO7vvA02",
        "outputId": "805a8f13-dc9c-4375-af67-93788d37c2dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I  [] 0 [] 0\n",
            "live  ['I'] 1 ['on', 'but', 'used'] 3\n",
            "on  [] 0 ['Road'] 1\n",
            "Greaves FAC [] 0 [] 0\n",
            "Road FAC ['Greaves'] 1 ['in'] 1\n",
            "in  [] 0 ['Lancaster'] 1\n",
            "Lancaster GPE [] 0 [] 0\n",
            "but  [] 0 [] 0\n",
            "I  [] 0 [] 0\n",
            "used  ['I'] 1 ['live'] 1\n",
            "to  [] 0 [] 0\n",
            "live  ['to'] 1 ['on'] 1\n",
            "on  [] 0 ['Street'] 1\n",
            "Greenfield FAC [] 0 [] 0\n",
            "Street FAC ['Greenfield'] 1 [] 0\n",
            "0\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://stackoverflow.com/questions/57206701/spacy-tokenizer-rule-for-exceptions-that-contain-whitespace suggests a couple of approaches. The simplest might be to merge noun chunks?"
      ],
      "metadata": {
        "id": "-WYGwYCWIuhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp.add_pipe(\"merge_noun_chunks\")\n",
        "doc = nlp(u'I live on Greaves Road')\n",
        "for token in doc:\n",
        "  print(token.text, token.pos_, token.dep_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_BC1mBQJ0PY",
        "outputId": "e674b624-a849-4312-a250-25c2f0abb2cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I PRON nsubj\n",
            "live VERB ROOT\n",
            "on ADP prep\n",
            "Greaves Road PROPN pobj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding a terminology list to our model"
      ],
      "metadata": {
        "id": "QhKnsea2y_pK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thanks to https://notebook.community/psychemedia/parlihacks/notebooks/Text%20Scraping%20-%20Notes this might finally be the approach for pre-loading our additional location entries! The next step is (I think) to turn this into a custom pipeline that we can load alongside the proper one rather than doing the list munging each time"
      ],
      "metadata": {
        "id": "IJD2KxcAzGHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "from spacy import displacy\n",
        "from spacy.pipeline import EntityRuler\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# make sure we load and run our custom pipeline before the standard one\n",
        "# else our entities get overwritten\n",
        "ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
        "\n",
        "# make a set of patterns from the csv\n",
        "# and then add them to our custom pipeline\n",
        "# note this relies on top50_list.csv being loaded into the notebook session\n",
        "opennames = pd.read_csv('top50_list.csv')\n",
        "#opennames = pd.read_csv('osopennames_singlecolumn_2.csv', nrows=50)\n",
        "opennames_list= opennames[\"THING\"].tolist()\n",
        "#opennames_list= opennames.iloc[:,1].to_list()\n",
        "patterns=[]\n",
        "for t in opennames_list:\n",
        "  patterns.append({\"label\":\"GPE\", \"pattern\":t})\n",
        "print(len(patterns))\n",
        "ruler.add_patterns(patterns)\n",
        "\n",
        "# test with a doc that has some entries from the list\n",
        "# and some that are not\n",
        "# and use displacy to render the text showing the entities\n",
        "doc = nlp(\"BN21 1AA is in my csv along with Saxon Ground, St Aubyn's Road and A2040. London is not, and Richard Burton is certainly not!\")\n",
        "displacy.render(doc, jupyter=True, style='ent')\n"
      ],
      "metadata": {
        "id": "Nf07h2-IzOmO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "e871f6a2-5d97-4c51-c308-0c18bb599971"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    BN21 1AA\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " is in my csv along with \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Saxon Ground\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    St Aubyn's Road\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " and \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    A2040\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ". \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    London\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " is not, and \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Richard Burton\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " is certainly not!</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we need to test this out with the whole csv, which has now been committed to the github repository at https://github.com/archaeogeek/colab, so let's download it, check which folder we're in and that it has the right sort of data in it"
      ],
      "metadata": {
        "id": "YJ1b0It0dREF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/archaeogeek/colab/master/osopennames_singlecolumn_2.csv\n",
        "!pwd\n",
        "!head -n 10 osopennames_singlecolumn_2.csv"
      ],
      "metadata": {
        "id": "DFLl1PZudQcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the new model to disk"
      ],
      "metadata": {
        "id": "v2NLihYzgmTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "from spacy import displacy\n",
        "from spacy.pipeline import EntityRuler\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\", enable=[\"ner\"])\n",
        "# make sure we load and run our custom pipeline before the standard one\n",
        "# else our entities get overwritten\n",
        "ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
        "\n",
        "patterns=[]\n",
        "#for i in range(0,3000000,1000000):\n",
        "  # add osopennames in chunks\n",
        "#osopennames = pd.read_csv('osopennames_singlecolumn_2.csv', nrows=1000000, skiprows=i)\n",
        "osopennames = pd.read_csv('osopennames_singlecolumn_2.csv')\n",
        "o_list = osopennames.iloc[:,1]                        \n",
        "for t in o_list:\n",
        "  patterns.append({\"label\":\"GPE\", \"pattern\":t})                     \n",
        "print(len(patterns))\n",
        "ruler.add_patterns(patterns)\n",
        "nlp.to_disk(\"osopennames_ner\")\n",
        "\n",
        "# test with a doc that has some entries from the list\n",
        "# and some that are not\n",
        "# and use displacy to render the text showing the entities\n",
        "#doc = nlp(\"Gloup is in my csv along with Westing. ZE2 9EH is in my csv along with Saxon Ground, Doos' Geo and A2040. Los Angeles is not, and Richard Burton is certainly not!\")\n",
        "#displacy.render(doc, jupyter=True, style='ent')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3855350-f6a6-4879-cd9c-2dd735363c92",
        "id": "YKoLiETOJH9B"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2973899\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading our custom model and actually using it. We're almost there but have to figure out how to re-add the `en_core_web_sm` model *and* our custom pipeline"
      ],
      "metadata": {
        "id": "DskPuRkhgrZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "nlp = spacy.load(\"osopennames_ner\")\n",
        "print(nlp.pipe_names)\n",
        "doc = nlp(\"Gloup is in my csv along with Westing. ZE2 9EH is in my csv along with Saxon Ground, Doos' Geo and A2040. Los Angeles is not, and Richard Burton is certainly not!\")\n",
        "displacy.render(doc, jupyter=True, style='ent')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "xTnQJDlDdnas",
        "outputId": "426e3e80-4a87-44e4-dbf0-76e604cc7642"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['entity_ruler', 'ner']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Gloup\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " is in my csv along with \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Westing\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ". \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    ZE2 9EH\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " is in my csv along with \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Saxon Ground\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Doos' Geo\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " and \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    A2040\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ". \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Los Angeles\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " is not, and Richard \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Burton\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " is certainly not!</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}